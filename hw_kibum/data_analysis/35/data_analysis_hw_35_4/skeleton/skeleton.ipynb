{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-HJpxWuZZLE"
   },
   "source": [
    "## Data Programming - Spark: 인기 영화 찾기\n",
    "\n",
    "### 실습 목표\n",
    "\n",
    "MovieLens 데이터셋을 사용하여 인기 있는 영화를 찾고, Repartition Join과 Broadcast Join의 성능을 비교합니다.\n",
    "\n",
    "**참고:**\n",
    "\n",
    "* `../data/ml-100k/u.data` 와 `../data/ml-100k/u.item` 파일 경로를 실제 파일 경로로 변경해야 합니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Colab 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.2.4-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findspark를 사용해 Spark 초기화\n",
    "import findspark\n",
    "findspark.init(\"/content/spark-3.2.4-bin-hadoop3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0SKZv2NyZZLM"
   },
   "source": [
    "### 1. Repartition Join\n",
    "\n",
    "#### 개념 설명\n",
    "\n",
    "Repartition Join은 데이터를 키로 분할하여 조인하는 방법입니다.  `join()` 함수는 두 RDD를 영화 ID를 기준으로 결합합니다.\n",
    "\n",
    "#### Spark 설정 및 컨텍스트 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: 참고 주석과 pandas에 대한 지식을 기반으로 빈칸을 채워주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "O_6SgXYVZZLM"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession 생성\n",
    "# SparkSession은 DataFrame과 SQL 작업의 메인 엔트리 포인트입니다. 'PopularMovies_Repartition'라는 이름으로 세션을 생성합니다.\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"PopularMovies_Repartition\").getOrCreate()\n",
    "\n",
    "# SparkContext에 접근이 필요한 경우, SparkSession에서 sparkContext 속성을 통해 접근할 수 있습니다.\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gomlaoi3ZZLN"
   },
   "source": [
    "#### 데이터 불러오기 및 전처리\n",
    "\n",
    "1. **u.data 파일**: 영화 평가 수 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KGmyVHMZZLN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|MovieID|count|\n",
      "+-------+-----+\n",
      "|    496|  231|\n",
      "|    471|  221|\n",
      "|    463|   71|\n",
      "|    148|  128|\n",
      "|   1342|    2|\n",
      "|    833|   49|\n",
      "|   1088|   13|\n",
      "|   1591|    6|\n",
      "|   1238|    8|\n",
      "|   1580|    1|\n",
      "|   1645|    1|\n",
      "|    392|   68|\n",
      "|    623|   39|\n",
      "|    540|   43|\n",
      "|    858|    3|\n",
      "|    737|   59|\n",
      "|    243|  132|\n",
      "|   1025|   44|\n",
      "|   1084|   21|\n",
      "|   1127|   11|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# u.data 파일을 읽고 스키마 정의\n",
    "# read.option(): 파일 읽기 시 옵션을 설정합니다.\n",
    "# schema: DataFrame의 스키마(데이터 타입 및 열 이름)를 지정하여 데이터의 구조를 정의합니다.\n",
    "# delimiter 옵션을 사용해 탭(\"\\t\")을 구분자로 설정합니다.\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html\n",
    "data = spark.read.option(\"delimiter\", \"\\t\").csv(\"./ml-100k/u.data\", schema=\"UserID INT, MovieID INT, Rating INT, Timestamp LONG\")\n",
    "\n",
    "# 영화 ID별 평가 수 집계\n",
    "# groupBy(\"column\"): 지정한 열을 기준으로 데이터를 그룹화합니다.\n",
    "# count(): 각 그룹에 속하는 데이터 개수를 계산하여 새 열(count)을 생성합니다.\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.groupBy.html\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.count.html\n",
    "movieCounts = data.__________(\"MovieID\").count()\n",
    "\n",
    "# 결과 출력\n",
    "# show(n): DataFrame의 상위 n개 행을 출력합니다. 기본값은 20개이며, n 값을 조정할 수 있습니다.\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html\n",
    "movieCounts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOqoet0lZZLO"
   },
   "source": [
    "2. **u.item 파일**: 영화 ID와 이름 매칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnxI7q85ZZLO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|MovieID|               Title|\n",
      "+-------+--------------------+\n",
      "|      1|    Toy Story (1995)|\n",
      "|      2|    GoldenEye (1995)|\n",
      "|      3|   Four Rooms (1995)|\n",
      "|      4|   Get Shorty (1995)|\n",
      "|      5|      Copycat (1995)|\n",
      "|      6|Shanghai Triad (Y...|\n",
      "|      7|Twelve Monkeys (1...|\n",
      "|      8|         Babe (1995)|\n",
      "|      9|Dead Man Walking ...|\n",
      "|     10|  Richard III (1995)|\n",
      "|     11|Seven (Se7en) (1995)|\n",
      "|     12|Usual Suspects, T...|\n",
      "|     13|Mighty Aphrodite ...|\n",
      "|     14|  Postino, Il (1994)|\n",
      "|     15|Mr. Holland's Opu...|\n",
      "|     16|French Twist (Gaz...|\n",
      "|     17|From Dusk Till Da...|\n",
      "|     18|White Balloon, Th...|\n",
      "|     19|Antonia's Line (1...|\n",
      "|     20|Angels and Insect...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# u.item 파일을 읽고 스키마 정의\n",
    "# read.option(): 파일을 읽을 때 특정 옵션을 설정합니다. 여기서는 \"delimiter\" 옵션을 사용하여 필드 구분자로 \"|\"을 지정합니다.\n",
    "# schema: 스키마를 지정하여 각 열의 데이터 타입을 명시합니다. MovieID는 정수형, Title은 문자열로 설정합니다.\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html\n",
    "item_df = spark.read.option(\"delimiter\", \"|\").csv(\"./ml-100k/u.item\", schema=\"MovieID INT, Title STRING\")\n",
    "\n",
    "# 필요한 열만 선택하여 (MovieID, Title) 형식의 DataFrame 생성\n",
    "# select(\"column1\", \"column2\", ...): DataFrame에서 필요한 열만 선택하여 새로운 DataFrame을 생성합니다.\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.select.html\n",
    "id_name_df = item_df.__________(\"__________\", \"__________\")\n",
    "\n",
    "# 결과 출력\n",
    "id_name_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YEVZw7YNZZLP"
   },
   "source": [
    "3. **조인 수행**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhX7EojTZZLP"
   },
   "outputs": [],
   "source": [
    "# 두 DataFrame을 MovieID 기준으로 조인\n",
    "# join(other, on): 두 DataFrame을 조인할 때 사용됩니다. `on` 파라미터로 조인할 열을 지정하며, 같은 MovieID 값을 기준으로 조인합니다.\n",
    "# 결과로 영화 제목(Title)과 평가 수(count) 열을 포함하는 DataFrame을 생성합니다.\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.join.html\n",
    "results = movieCounts.__________(id_name_df, on=\"__________\").select(\"Title\", \"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iozUZ_SZZLQ"
   },
   "source": [
    "#### 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "ZqlocoufZZLQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(Title=\"It's a Wonderful Life (1946)\", count=231)\n",
      "Row(Title='Courage Under Fire (1996)', count=221)\n",
      "Row(Title='Secret of Roan Inish, The (1994)', count=71)\n",
      "Row(Title='Ghost and the Darkness, The (1996)', count=128)\n",
      "Row(Title='Convent, The (Convento, O) (1995)', count=2)\n",
      "Row(Title='Bulletproof (1996)', count=49)\n",
      "Row(Title='Double Team (1997)', count=13)\n",
      "Row(Title='Duoluo tianshi (1995)', count=6)\n",
      "Row(Title='Full Speed (1996)', count=8)\n",
      "Row(Title='Liebelei (1933)', count=1)\n",
      "Row(Title='Butcher Boy, The (1998)', count=1)\n",
      "Row(Title='Man Without a Face, The (1993)', count=68)\n",
      "Row(Title='Angels in the Outfield (1994)', count=39)\n",
      "Row(Title='Money Train (1995)', count=43)\n",
      "Row(Title='Amityville: Dollhouse (1996)', count=3)\n",
      "Row(Title='Sirens (1994)', count=59)\n",
      "Row(Title='Jungle2Jungle (1997)', count=132)\n",
      "Row(Title='Fire Down Below (1997)', count=44)\n",
      "Row(Title='Anne Frank Remembered (1995)', count=21)\n",
      "Row(Title='Truman Show, The (1998)', count=11)\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력\n",
    "for result in results.collect()[:20]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y1wnl_kzZZLQ"
   },
   "source": [
    "#### Repartition Join 요약\n",
    "\n",
    "* 데이터를 재분배해야 하므로 대규모 데이터셋에서는 성능 비용이 발생할 수 있습니다.\n",
    "* 데이터가 매우 큰 경우 비효율적일 수 있지만, 모든 키가 적절히 분산되어 있으면 사용할 수 있습니다.\n",
    "\n",
    "\n",
    "### 2. Broadcast Join\n",
    "\n",
    "#### 개념 설명\n",
    "\n",
    "Broadcast Join은 작은 데이터를 클러스터의 모든 노드에 복사(브로드캐스트)하여 효율적인 조인을 수행하는 방법입니다.\n",
    "\n",
    "#### Spark 설정 및 컨텍스트 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: 참고 주석과 pandas에 대한 지식을 기반으로 빈칸을 채워주세요."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TpVQDcUMZZLR"
   },
   "source": [
    "#### 영화 이름 로드 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "vnt0h4ImZZLR"
   },
   "outputs": [],
   "source": [
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"./ml-100k/u.item\") as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "# 영화 이름 딕셔너리 생성 및 브로드캐스트\n",
    "# spark.sparkContext.broadcast(value): Spark 애플리케이션 전체에 걸쳐 공유할 수 있는 변수를 생성합니다.\n",
    "# `broadcast` 변수는 각 워커 노드에 저장되어 네트워크 오버헤드를 줄입니다. 여기서는 loadMovieNames() 함수에서 생성된 영화 이름 딕셔너리를 브로드캐스트 변수로 만듭니다.\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#pyspark.SparkContext.broadcast\n",
    "nameDict = spark.sparkContext.broadcast(loadMovieNames())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_0Mss6XZZLS"
   },
   "source": [
    "#### 데이터 불러오기 및 전처리\n",
    "\n",
    "1. **u.data 파일**: 영화 평가 수 집계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwDwJoWzZZLS"
   },
   "outputs": [],
   "source": [
    "# u.data 파일을 DataFrame으로 로드하고 스키마 정의\n",
    "# read.option(\"key\", \"value\"): 파일을 읽을 때 특정 옵션을 설정합니다. 여기서는 필드 구분자를 탭(\"\\t\")으로 설정합니다.\n",
    "# csv(path, schema): 지정된 경로의 CSV 파일을 읽고, 스키마를 정의하여 각 열의 데이터 타입을 명시합니다.\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html\n",
    "data_df = spark.read.option(\"delimiter\", \"\\t\").csv(\"./ml-100k/u.data\", schema=\"UserID INT, MovieID INT, Rating INT, Timestamp LONG\")\n",
    "\n",
    "# 영화 ID별 평가 수를 집계\n",
    "# groupBy(\"column\"): 지정한 열을 기준으로 데이터를 그룹화합니다. 여기서는 MovieID 열로 그룹화합니다.\n",
    "# count(): 그룹별로 속한 데이터 개수를 세어 count 열을 생성합니다.\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.groupBy.html\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.GroupedData.count.html\n",
    "movieCounts = data_df.__________(\"MovieID\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Be71bm3BZZLS"
   },
   "source": [
    "2. **영화 이름과 조인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWvL88OIZZLT"
   },
   "outputs": [],
   "source": [
    "# 영화 이름을 브로드캐스트 변수에서 가져오는 UDF 정의\n",
    "# udf(f, returnType): 사용자 정의 함수를 Spark UDF로 등록하여 DataFrame 열에 적용할 수 있게 합니다.\n",
    "# 여기서는 영화 ID를 받아서 해당 영화 이름을 반환하는 함수 getMovieName을 UDF로 등록합니다.\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.functions.udf.html\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# 영화 ID로 영화 이름을 검색하는 함수 정의 및 UDF 생성\n",
    "# nameDict.value.get(movie_id, \"Unknown\"): Broadcast 변수를 사용하여 주어진 영화 ID에 해당하는 영화 이름을 가져옵니다.\n",
    "# 만약 해당 영화 ID가 존재하지 않으면 기본값 \"Unknown\"을 반환합니다.\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.html#pyspark.Broadcast.value\n",
    "def getMovieName(movie_id):\n",
    "    return nameDict.value.get(movie_id, \"Unknown\")\n",
    "\n",
    "# StringType: UDF 반환 타입을 문자열로 지정합니다.\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.types.StringType.html\n",
    "getMovieNameUDF = __________(getMovieName, StringType())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 ID별 평가 수에 영화 이름 추가\n",
    "# withColumn(colName, col): 새로운 열을 추가하거나 기존 열을 업데이트합니다. 여기서는 \"MovieName\"이라는 새 열을 추가하고, 영화 ID를 기반으로 영화 이름을 가져오는 UDF를 적용합니다.\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.withColumn.html\n",
    "results = movieCounts.__________(\"MovieName\", getMovieNameUDF(col(\"__________\"))).select(\"MovieName\", \"count\")\n",
    "\n",
    "# select(\"column1\", \"column2\"): 필요한 열만 선택하여 최종 결과를 생성합니다. 여기서는 \"MovieName\"과 \"count\" 열을 선택합니다.\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.select.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZhhSXZCZZLT"
   },
   "source": [
    "#### 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Uzf423gEZZLT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(MovieName=\"It's a Wonderful Life (1946)\", count=231)\n",
      "Row(MovieName='Courage Under Fire (1996)', count=221)\n",
      "Row(MovieName='Secret of Roan Inish, The (1994)', count=71)\n",
      "Row(MovieName='Ghost and the Darkness, The (1996)', count=128)\n",
      "Row(MovieName='Convent, The (Convento, O) (1995)', count=2)\n",
      "Row(MovieName='Bulletproof (1996)', count=49)\n",
      "Row(MovieName='Double Team (1997)', count=13)\n",
      "Row(MovieName='Duoluo tianshi (1995)', count=6)\n",
      "Row(MovieName='Full Speed (1996)', count=8)\n",
      "Row(MovieName='Liebelei (1933)', count=1)\n",
      "Row(MovieName='Butcher Boy, The (1998)', count=1)\n",
      "Row(MovieName='Man Without a Face, The (1993)', count=68)\n",
      "Row(MovieName='Angels in the Outfield (1994)', count=39)\n",
      "Row(MovieName='Money Train (1995)', count=43)\n",
      "Row(MovieName='Amityville: Dollhouse (1996)', count=3)\n",
      "Row(MovieName='Sirens (1994)', count=59)\n",
      "Row(MovieName='Jungle2Jungle (1997)', count=132)\n",
      "Row(MovieName='Fire Down Below (1997)', count=44)\n",
      "Row(MovieName='Anne Frank Remembered (1995)', count=21)\n",
      "Row(MovieName='Truman Show, The (1998)', count=11)\n"
     ]
    }
   ],
   "source": [
    "# 결과 출력\n",
    "for result in results.collect()[:20]:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EOp604TIZZLT"
   },
   "source": [
    "#### Broadcast Join 요약\n",
    "\n",
    "* 작은 데이터셋을 모든 노드에 복사하여 더 효율적인 조인을 제공합니다.\n",
    "* 대규모 데이터를 조인할 때 성능을 크게 향상시킬 수 있습니다.\n",
    "\n",
    "\n",
    "### 결론 및 비교\n",
    "\n",
    "* **Repartition Join**: 데이터가 클수록 성능이 저하될 수 있으며, 재분배 비용이 큽니다. 그러나 모든 데이터를 고르게 분산시키는 데 적합합니다.\n",
    "* **Broadcast Join**: 작은 데이터셋을 브로드캐스트하면 성능이 크게 향상됩니다. 대규모 데이터를 조인할 때 효율적입니다.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
