{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Pyspark in Colab\n",
    "\n",
    "## Spark 설치 및 초기 설정\n",
    " Spark은 대용량 데이터 처리를 위한 분산 컴퓨팅 프레임워크입니다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo apt-get install -y openjdk-8-jdk-headless -qq > /dev/null\n",
    "!wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.2.4-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findspark를 사용해 Spark 초기화\n",
    "import findspark\n",
    "findspark.init(\"/content/spark-3.2.4-bin-hadoop3.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SparkSession과 SparkContext를 설정합니다.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# SparkSession 생성\n",
    "spark = SparkSession.builder.appName(\"Friends Data Analysis\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터 불러오기 및 SQL 연습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: 참고 주석과 pandas에 대한 지식을 기반으로 빈칸을 채워주세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 스키마 설정\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "# StructType과 StructField는 데이터 스키마를 정의할 때 사용됩니다.\n",
    "# 참조: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StructType\n",
    "\n",
    "# SparkSession 생성\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "# 기존 세션을 사용하거나 새로운 세션을 생성합니다.\n",
    "\n",
    "# 데이터 읽기 옵션 설정\n",
    "option1 = \"header\"; argument1 = \"False\"  # CSV 파일에 헤더가 포함되지 않았음을 명시\n",
    "option2 = \"inferSchema\"; argument2 = \"true\"  # 데이터 타입을 자동으로 추론하도록 설정\n",
    "option3 = \"delimiter\"; argument3 = \",\"  # 쉼표를 구분자로 지정\n",
    "\n",
    "# 스키마 정의\n",
    "fields = [\n",
    "    ___________(\"index\", IntegerType(), True),  # 인덱스 열 정의\n",
    "    ___________(\"name\", StringType(), True),  # 이름 열 정의\n",
    "    ___________(\"age\", IntegerType(), True),  # 나이 열 정의\n",
    "    ___________(\"numOfFriends\", IntegerType(), True)  # 친구 수 열 정의\n",
    "]\n",
    "schema = StructType(fields)  # 정의된 필드로 스키마 생성\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = spark.read.schema(schema).format(\"csv\")\\\n",
    "    .option(option1, argument1)\\\n",
    "    .option(option2, argument2)\\\n",
    "    .option(option3, argument3)\\\n",
    "    .load(\"fakefriends.csv\")\n",
    "# read.schema()는 스키마를 설정하여 CSV 파일을 읽는 메서드입니다.\n",
    "# 참조: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader\n",
    "\n",
    "# 특정 열 선택 및 데이터 보기\n",
    "# 참조: https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.select\n",
    "df.select(\"name\", \"age\").show()  # name과 age 열만 선택하여 표시\n",
    "# select()는 특정 열을 선택하여 반환하는 메서드입니다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show()는 DataFrame을 콘솔에 표시합니다.\n",
    "# 참조: https://downloads.apache.org/spark/docs/3.2.4/api/python/reference/api/pyspark.sql.DataFrame.html\n",
    "df.show()  # 데이터 미리보기\n",
    "df.take(10)  # 상위 10개 행 가져오기\n",
    "df.printSchema()  # 스키마 출력\n",
    "df.describe().show()  # 요약 통계\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정렬 및 중복 제거\n",
    "# sort()는 지정된 열을 기준으로 DataFrame을 정렬합니다.\n",
    "# 참조: https://downloads.apache.org/spark/docs/3.2.4/api/python/reference/api/pyspark.sql.DataFrame.sort\n",
    "df.___________(\"name\").show()  # 이름 기준으로 정렬\n",
    "\n",
    "# dropDuplicates()는 중복 행을 제거합니다.\n",
    "unique_df = df.___________()  # 중복 제거\n",
    "\n",
    "unique_df.___________(\"name\").show()  # 중복 제거된 데이터 정렬\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그룹화 및 집계\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "# groupby()는 지정된 열을 기준으로 데이터를 그룹화합니다.\n",
    "# agg()는 집계 함수를 적용합니다.\n",
    "# 참조: https://downloads.apache.org/spark/docs/3.2.4/api/python/reference/api/pyspark.sql.DataFrame.groupBy\n",
    "df_group = unique_df.___________('age').___________(sf.count('*').alias('count')).sort('age')\n",
    "df_group.show()  # 나이별로 그룹화하고 카운트 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL 쿼리 실행\n",
    "df.___________(\"people\")  # people이라는 이름의 임시 뷰 생성\n",
    "# createOrReplaceTempView()는 DataFrame을 SQL 테이블처럼 사용할 수 있는 임시 뷰로 만듭니다.\n",
    "# 참조: https://downloads.apache.org/spark/docs/3.2.4/api/python/reference/api/pyspark.sql.DataFrame.createOrReplaceTempView.html\n",
    "\n",
    "spark.___________(\"SELECT name, count(*) FROM people WHERE name>'J' GROUP BY name\").show()  # SQL 쿼리 실행\n",
    "# sql() 메서드는 SQL 쿼리를 실행하고 결과를 DataFrame으로 반환합니다.\n",
    "# 참조: https://downloads.apache.org/spark/docs/3.2.4/api/python/reference/api/pyspark.sql.SparkSession.sql.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 열 생성 및 열 이름 변경\n",
    "df_sub = df.___________(\"substring\", df[\"name\"].substr(0, 1))  # name 열의 첫 글자를 substring 열로 추가\n",
    "# withColumn()은 DataFrame에 새로운 열을 추가하거나 기존 열을 수정하는 메서드입니다.\n",
    "# substr()는 문자열의 서브스트링을 반환합니다.\n",
    "df_sub.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# withColumnRenamed()는 기존 열의 이름을 변경합니다.\n",
    "# 참조: https://downloads.apache.org/spark/docs/3.2.4/api/python/reference/api/pyspark.sql.DataFrame.withColumnRenamed.html\n",
    "df_initial = df_sub.___________(\"substring\", \"initial\")  # substring 열을 initial로 이름 변경\n",
    "df_initial.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 열 삭제\n",
    "df_drop = df_initial.___________(\"age\")  # age 열 제거\n",
    "# drop()은 지정된 열을 DataFrame에서 제거합니다.\n",
    "# 참조: https://downloads.apache.org/spark/docs/3.2.4/api/python/reference/pyspark.pandas/api/pyspark.pandas.DataFrame.drop.html\n",
    "\n",
    "df_drop.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD 변환 및 Pandas 변환\n",
    "rdd_out = df_initial.rdd  # DataFrame을 RDD로 변환\n",
    "# rdd 속성은 DataFrame을 RDD 형식으로 변환합니다.\n",
    "print(rdd_out.collect()[:10])  # RDD 출력\n",
    "\n",
    "pandas_out = df_initial.toPandas() # RDD의 상위 10개 데이터 출력\n",
    "# collect()는 RDD 또는 DataFrame의 모든 요소를 Python 목록으로 반환합니다.\n",
    "print(pandas_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()  # Spark 세션 중지"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
