{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HN6naN-OcLyO"
   },
   "source": [
    "# Data Programming - Spark: 타이타닉 데이터 분석\n",
    "\n",
    "### 1. 데이터를 위한 Spark SQL 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Colab에서 환경 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: 아래의 코드를 따라치면서 Colab 환경에서 spark ml을 실행할 준비를 하고 연결을 확인합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzYepFdOcLyT"
   },
   "outputs": [],
   "source": [
    "!wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.2.4-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findspark를 사용해 Spark 초기화\n",
    "import findspark\n",
    "findspark.init(\"/content/spark-3.2.4-bin-hadoop3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LbSyhGfdcLyX"
   },
   "source": [
    "### 2. 데이터 분석을 위한 Spark SQL 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezh_eYvVcLyZ"
   },
   "source": [
    "#### CSV 파일 읽어 DataFrame 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: 아래의 주석과 참고페이지를 기반으로 빈칸을 채워주세요. 타입과 필드를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YtxU3x12cLyZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType\n",
    "\n",
    "# SparkSession 설정\n",
    "# 'Titanic Advanced Analysis'라는 이름의 Spark 세션을 생성합니다.\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.SparkSession.html\n",
    "spark = SparkSession.builder.appName(\"Titanic Advanced Analysis\").getOrCreate()\n",
    "\n",
    "# 스키마 정의\n",
    "# 데이터 스키마를 명시적으로 정의하여 각 열의 데이터 타입을 지정합니다.\n",
    "# 이 스키마는 CSV 파일을 읽을 때 적용됩니다.\n",
    "schema = ___________([\n",
    "    ___________(\"PassengerId\", ___________(), True),   # 승객 ID (int)\n",
    "    ___________(\"Survived\", ___________(), True),      # 생존 여부 (0 = 사망, 1 = 생존) (int)\n",
    "    ___________(\"Pclass\", ___________(), True),        # 승객 등급 (1, 2, 3) (int)\n",
    "    ___________(\"Name\", ___________(), True),           # 승객 이름 (String)\n",
    "    ___________(\"Sex\", ___________(), True),            # 성별 (String)\n",
    "    ___________(\"Age\", ___________(), True),             # 나이 (float)\n",
    "    ___________(\"SibSp\", ___________(), True),         # 동반한 형제/배우자 수 (int)\n",
    "    ___________(\"Parch\", ___________(), True),         # 동반한 부모/자녀 수 (int)\n",
    "    ___________(\"Ticket\", ___________(), True),         # 티켓 번호 (String)\n",
    "    ___________(\"Fare\", ___________(), True),            # 운임 요금  (float)\n",
    "    ___________(\"Cabin\", ___________(), True),          # 객실 번호 (String)\n",
    "    ___________(\"Embarked\", ___________(), True)        # 탑승 항구 (C = Cherbourg, Q = Queenstown, S = Southampton) (String)\n",
    "])\n",
    "\n",
    "# CSV 파일 읽기\n",
    "# 정의된 스키마와 옵션을 사용하여 CSV 파일을 읽어 DataFrame으로 로드합니다.\n",
    "# header 옵션은 첫 줄을 헤더로 사용할지 여부를 설정하며, delimiter 옵션은 필드 구분자를 지정합니다.\n",
    "# 참고: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.csv.html\n",
    "df = spark.read.schema(schema).format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"delimiter\", \",\")\\\n",
    "    .load(\"../data/titanic.csv\")\n",
    "\n",
    "# 데이터 출력\n",
    "# show() 메서드를 사용하여 상위 5개 행을 출력하여 데이터를 확인합니다.\n",
    "df.show(5)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
