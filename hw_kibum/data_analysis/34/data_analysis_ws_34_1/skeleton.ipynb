{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `TODO`: Spark를 활용하기 위해 필요한 환경을 설정합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 해당 설명은 리눅스 기준입니다. 해당 환경이 불가능한 경우 아래의 Colab 환경에서 진행하면 됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Java 설치 및 환경 설정\n",
    "- sudo apt update  # 패키지 목록을 업데이트합니다.\n",
    "- sudo apt install openjdk-11-jdk  # OpenJDK 11 버전을 설치합니다.\n",
    "\n",
    "#### Java 설치가 완료된 후, 설치 확인\n",
    "- java -version  # Java 설치가 성공적으로 되었는지 확인합니다.\n",
    "\n",
    "#### Java 환경 변수 설정\n",
    "- export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64  # Java 설치 경로를 환경 변수에 설정합니다.\n",
    "- export PATH=$JAVA_HOME/bin:$PATH  # Java 경로를 시스템 PATH에 추가합니다.\n",
    "- echo \"export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64\" >> ~/.bashrc  # bashrc 파일에 JAVA_HOME 설정을 추가합니다.\n",
    "- echo \"export PATH=\\$JAVA_HOME/bin:\\$PATH\" >> ~/.bashrc  # bashrc 파일에 PATH 설정을 추가합니다.\n",
    "- source ~/.bashrc  # bashrc 파일을 다시 불러와 환경 변수를 적용합니다.\n",
    "- echo $JAVA_HOME  # JAVA_HOME 경로가 제대로 설정되었는지 확인합니다.\n",
    "\n",
    "#### Spark 다운로드 및 설치\n",
    "- sudo wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz  # Spark 3.2.4 버전을 다운로드합니다.\n",
    "- sudo tar xf spark-3.2.4-bin-hadoop3.2.tgz  # 다운로드한 tar 파일을 해제합니다.\n",
    "- sudo mv spark-3.2.4-bin-hadoop3.2/ /opt/spark  # Spark 디렉토리를 /opt/spark로 이동합니다.\n",
    "\n",
    "#### Spark 환경 변수 설정\n",
    "- export SPARK_HOME=/opt/spark  # Spark 설치 경로를 환경 변수에 설정합니다.\n",
    "- export PATH=$SPARK_HOME/bin:$SPARK_HOME/sbin:$PATH  # Spark 경로를 시스템 PATH에 추가합니다.\n",
    "- echo \"export SPARK_HOME=/opt/spark\" >> ~/.bashrc  # bashrc 파일에 SPARK_HOME 설정을 추가합니다.\n",
    "- echo \"export PATH=\\$SPARK_HOME/bin:\\$SPARK_HOME/sbin:\\$PATH\" >> ~/.bashrc  # bashrc 파일에 PATH 설정을 추가합니다.\n",
    "- source ~/.bashrc  # bashrc 파일을 다시 불러와 환경 변수를 적용합니다.\n",
    "\n",
    "#### Spark 설치 확인\n",
    "- echo $SPARK_HOME  # SPARK_HOME 경로가 제대로 설정되었는지 확인합니다.\n",
    "- spark-shell --version  # Spark 설치가 성공적으로 되었는지 확인합니다.\n",
    "\n",
    "#### PySpark 및 findspark 설치\n",
    "- pip install pyspark==3.2.4  # PySpark 3.2.4 버전을 설치합니다.\n",
    "- pip install -q findspark  # findspark를 설치하여 PySpark를 쉽게 사용할 수 있도록 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Java 및 Spark 환경 변수 설정\n",
    "os.environ[\"SPARK_HOME\"] = \"/opt/spark\"  # 압축 해제된 경로로 변경 -> 본인의 경로에 맞게 설정해야합니다.\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findspark를 사용해 Spark 환경 초기화\n",
    "import findspark\n",
    "\n",
    "# findspark는 Python 환경에서 PySpark를 사용할 수 있도록 Spark 경로를 자동으로 설정해주는 라이브러리입니다.\n",
    "# findspark.init()은 환경 변수를 설정해 PySpark가 올바르게 작동하도록 합니다.\n",
    "# init() 메서드에 경로를 지정하지 않으면, 기본적으로 SPARK_HOME 환경 변수를 자동으로 검색합니다.\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab 환경에서 환경설정\n",
    "- 앞으로의 남은 실습은 Colab에서 실행한다는 가정하에 실습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.2.4-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findspark를 사용해 Spark 환경 초기화\n",
    "import findspark\n",
    "\n",
    "# findspark는 Python 환경에서 PySpark를 사용할 수 있도록 Spark 경로를 자동으로 설정해주는 라이브러리입니다.\n",
    "# findspark.init()은 환경 변수를 설정해 PySpark가 올바르게 작동하도록 합니다.\n",
    "# init() 메서드에 경로를 지정하지 않으면, 기본적으로 SPARK_HOME 환경 변수를 자동으로 검색합니다.\n",
    "findspark.init(\"/content/spark-3.2.4-bin-hadoop3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 및 spark 시작 및 처리를 위한 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark 라이브러리를 임포트합니다. PySpark는 대규모 데이터를 병렬로 처리하고 분석하는 데 사용됩니다.\n",
    "import pyspark\n",
    "\n",
    "# DataFrame 및 SQL 작업을 수행하기 위해 SparkSession을 임포트합니다.\n",
    "# SparkSession은 PySpark에서 DataFrame 및 SQL 작업을 위한 진입점 역할을 합니다.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark 설정을 구성하고 SparkContext를 생성하기 위해 필요한 모듈을 임포트합니다.\n",
    "# SparkConf는 Spark 애플리케이션의 설정을 정의하는 데 사용되고,\n",
    "# SparkContext는 RDD API를 사용하여 데이터 처리를 수행하기 위한 기본 객체입니다.\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# 새로운 SparkSession을 생성합니다. 'test'는 애플리케이션 이름으로,\n",
    "# Spark UI 및 로그에 표시됩니다. SparkSession은 DataFrame과 SQL 작업을 수행하는 데 사용됩니다.\n",
    "spark = SparkSession.builder.appName('test').getOrCreate()\n",
    "\n",
    "# SparkContext 객체를 가져옵니다. SparkContext는 RDD API 작업을 수행할 때 사용되는 기본 객체입니다.\n",
    "# RDD(Resilient Distributed Dataset)는 PySpark의 저수준 API로, 데이터를 분산 처리하기 위해 사용됩니다.\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 위의 코드를 따라 치면서 python을 활용한 spark 사용 방법에 익숙해지세요!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
