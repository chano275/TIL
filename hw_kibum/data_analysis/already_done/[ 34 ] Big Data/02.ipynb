{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab 환경에서 환경설정\n",
    "- 앞으로의 남은 실습은 Colab에서 실행한다는 가정하에 실습을 진행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget -q https://archive.apache.org/dist/spark/spark-3.2.4/spark-3.2.4-bin-hadoop3.2.tgz\n",
    "!tar xf spark-3.2.4-bin-hadoop3.2.tgz\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findspark를 사용해 Spark 환경 초기화\n",
    "import findspark\n",
    "\n",
    "# findspark는 Python 환경에서 PySpark를 사용할 수 있도록 Spark 경로를 자동으로 설정해주는 라이브러리입니다.\n",
    "# findspark.init()은 환경 변수를 설정해 PySpark가 올바르게 작동하도록 합니다.\n",
    "# init() 메서드에 경로를 지정하지 않으면, 기본적으로 SPARK_HOME 환경 변수를 자동으로 검색합니다.\n",
    "findspark.init(\"/content/spark-3.2.4-bin-hadoop3.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 라이브러리 및 spark 시작 및 처리를 위한 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark 라이브러리를 임포트합니다. PySpark는 대규모 데이터를 병렬로 처리하고 분석하는 데 사용됩니다.\n",
    "import pyspark\n",
    "\n",
    "# DataFrame 및 SQL 작업을 수행하기 위해 SparkSession을 임포트합니다.\n",
    "# SparkSession은 PySpark에서 DataFrame 및 SQL 작업을 위한 진입점 역할을 합니다.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark 설정을 구성하고 SparkContext를 생성하기 위해 필요한 모듈을 임포트합니다.\n",
    "# SparkConf는 Spark 애플리케이션의 설정을 정의하는 데 사용되고,\n",
    "# SparkContext는 RDD API를 사용하여 데이터 처리를 수행하기 위한 기본 객체입니다.\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "# 새로운 SparkSession을 생성합니다. 'transformations'은 애플리케이션 이름으로,\n",
    "# Spark UI 및 로그에 표시됩니다. SparkSession은 DataFrame과 SQL 작업을 수행하는 데 사용됩니다.\n",
    "spark = SparkSession.builder.appName('transformations').getOrCreate()\n",
    "\n",
    "# SparkContext 객체를 가져옵니다. SparkContext는 RDD API 작업을 수행할 때 사용되는 기본 객체입니다.\n",
    "# RDD(Resilient Distributed Dataset)는 PySpark의 저수준 API로, 데이터를 분산 처리하기 위해 사용됩니다.\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Map 변환\n",
    "- 승객 데이터에서 Survived 여부와 승객의 나이를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Jack', 0, 25)], [('Rose', 1, 22), ('John', 1, 30)]]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# 타이타닉 데이터 RDD 생성: (\"이름\", 생존 여부, 나이) 형태의 데이터를 포함\n",
    "# 데이터는 2개의 파티션으로 나뉘어 분산 처리됨\n",
    "titanic_data = sc.parallelize([(\"Jack\", 0, 25), (\"Rose\", 1, 22), (\"John\", 1, 30)], 2)\n",
    "\n",
    "# RDD의 파티션 수를 가져옴\n",
    "num_partitions = titanic_data.getNumPartitions()\n",
    "\n",
    "# 각 파티션에 포함된 데이터를 확인\n",
    "print(titanic_data.glom().collect())\n",
    "print(num_partitions)\n",
    "\n",
    "# 출력 결과: 각 파티션의 데이터와 파티션 수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 위의 코드를 따라 치면서 python을 활용한 spark 사용 방법에 익숙해지세요!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Filter 변환\n",
    "- 생존한 승객만 필터링합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rose', 1, 22), ('John', 1, 30)]\n"
     ]
    }
   ],
   "source": [
    "# 생존 여부가 1(생존자)인 데이터만 필터링\n",
    "survived_filter = titanic_data.filter(lambda x: x[1] == 1)\n",
    "print(survived_filter.collect())\n",
    "\n",
    "# 출력 결과: 생존자 데이터만 포함된 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 위의 코드를 따라 치면서 python을 활용한 spark 사용 방법에 익숙해지세요!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GroupBy 변환\n",
    "- 이름의 첫 글자로 승객 그룹화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('J', [('Jack', 0, 25), ('John', 1, 30)]), ('R', [('Rose', 1, 22)])]\n"
     ]
    }
   ],
   "source": [
    "# 타이타닉 데이터 RDD를 이름의 첫 글자를 기준으로 그룹화\n",
    "grouped_by_initial = titanic_data.groupBy(lambda x: x[0][0])\n",
    "\n",
    "# 각 그룹의 키(이름의 첫 글자)와 해당 그룹에 속하는 데이터를 리스트로 변환하여 출력\n",
    "print([(t[0], list(t[1])) for t in grouped_by_initial.collect()])\n",
    "\n",
    "# 출력 결과: 이름의 첫 글자를 기준으로 그룹화된 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 위의 코드를 따라 치면서 python을 활용한 spark 사용 방법에 익숙해지세요!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GroupByKey 변환\n",
    "- 객실 등급별로 승객의 나이 정보 그룹화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, [25, 22]), (2, [30]), (3, [45])]\n"
     ]
    }
   ],
   "source": [
    "# 승객 데이터를 (클래스, 나이) 형식으로 생성\n",
    "passengers = sc.parallelize([(1, 25), (2, 30), (1, 22), (3, 45)])\n",
    "\n",
    "# 클래스별로 승객의 나이를 그룹화\n",
    "grouped_by_class = passengers.groupByKey()\n",
    "\n",
    "# 각 클래스와 해당 클래스의 나이 데이터를 리스트로 변환하여 출력\n",
    "print([(t[0], list(t[1])) for t in grouped_by_class.collect()])\n",
    "\n",
    "# 출력 결과: 클래스별로 그룹화된 나이 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 위의 코드를 따라 치면서 python을 활용한 spark 사용 방법에 익숙해지세요!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Word Counting Using ReduceByKey 변환\n",
    "- Survived 값으로 생존자와 비생존자 수 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 2)]\n"
     ]
    }
   ],
   "source": [
    "# 생존 여부를 키로, 값은 1로 매핑하여 생성 (0: 사망, 1: 생존)\n",
    "survived_counts = titanic_data.map(lambda x: (x[1], 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# 생존 여부별 승객 수를 출력\n",
    "print(survived_counts.collect())\n",
    "\n",
    "# 출력 결과: 생존 여부별 승객 수\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 위의 코드를 따라 치면서 python을 활용한 spark 사용 방법에 익숙해지세요!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Join 변환\n",
    "- 승객 이름과 객실 등급을 결합하여 join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:============================>                            (6 + 6) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Rose', (2, 1)), ('Jack', (1, 0))]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 승객 이름과 클래스 정보를 포함한 RDD 생성\n",
    "passengers_info = sc.parallelize([(\"Jack\", 1), (\"Rose\", 2)])\n",
    "\n",
    "# 승객 이름과 생존 여부 정보를 포함한 RDD 생성\n",
    "survival_info = sc.parallelize([(\"Jack\", 0), (\"Rose\", 1)])\n",
    "\n",
    "# 승객 이름을 기준으로 두 RDD를 조인\n",
    "joined_data = passengers_info.join(survival_info)\n",
    "\n",
    "# 조인된 데이터를 출력\n",
    "print(joined_data.collect())\n",
    "\n",
    "# 출력 결과: 승객 이름을 기준으로 병합된 데이터 (이름, (클래스, 생존 여부))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: 위의 코드를 따라 치면서 python을 활용한 spark 사용 방법에 익숙해지세요!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
